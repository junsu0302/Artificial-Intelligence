{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11005bf0",
   "metadata": {},
   "source": [
    "# 오차역전파법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ee3694",
   "metadata": {},
   "source": [
    "**오차역전파법(Back propagation)** 은 *가중치 매개변수의 기울기*를 효율적으로 계산하는 방법이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065301ee",
   "metadata": {},
   "source": [
    "## 1. 연쇄 법칙"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a20838",
   "metadata": {},
   "source": [
    "**연쇄 법칙(chain rule)** 은 전체 수식에서 *국소적 미분*을 전달하는 원리이다. 연쇄 법칙은 **합성 함수의 미분에 대한 성질**이다. 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508c353b",
   "metadata": {},
   "source": [
    "## 2. 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687be6a",
   "metadata": {},
   "source": [
    "### 2.1. 덧셈 노드의 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d10944",
   "metadata": {},
   "source": [
    "덧셈 노드의 역전파를 살펴보기 위해 $z = x + y$ 의 식을 살펴본다. 우선 $z = x + y$ 를 미분해보면 다음과 같다.\n",
    "\n",
    "$\\frac{\\theta z}{\\theta x} = 1$, $\\frac{\\theta z}{\\theta y} = 1$\n",
    "\n",
    "위와 같이 $\\frac{\\theta z}{\\theta x}$, $\\frac{\\theta z}{\\theta y}$는 모두 1이기 때문에 입력된 값을 그대로 다음 노드로 전송한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c825e6",
   "metadata": {},
   "source": [
    "### 2.2. 곱셈 노드의 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e79a3",
   "metadata": {},
   "source": [
    "곱셈 노드의 역전파를 살펴보기 위해 $z = xy$ 의 식을 살펴본다. 우선 $z = xy$ 를 미분해보면 다음과 같다.\n",
    "\n",
    "$\\frac{\\theta z}{\\theta x} = y$, $\\frac{\\theta z}{\\theta y} = x$\n",
    "\n",
    "곱셈 노드 역전파는 상류의 값에 순전파 때의 입력 신호들을 서로 바꾸어 곱하면 된다. \n",
    "\n",
    "곱셈의 역전파는 순방향 입력 신호 값이 필요하기 때문에 순전파의 입력 신호를 변수에 저장해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0e651",
   "metadata": {},
   "source": [
    "## 3. 역전파 단순 계층 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa8935",
   "metadata": {},
   "source": [
    "### 3.1. 덧셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cab8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ef878",
   "metadata": {},
   "source": [
    "1. **init** : 초기화할 필요가 없으니 pass\n",
    "2. **forward** : 입력받은 x, y를 더하여 반환\n",
    "3. **backward** : 상류에서 내려온 미분을 그대로 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d84ec",
   "metadata": {},
   "source": [
    "### 3.2. 곱셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ae1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6b47e3",
   "metadata": {},
   "source": [
    "1. **init** : 인스턴스 변수 x, y 초기화\n",
    "2. **forward** : 입력받은 x, y를 곱하여 반환\n",
    "3. **backward** : 상류에서 내려온 미분에 순전파의 값을 바꾸어 곱한 후 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe7c4f",
   "metadata": {},
   "source": [
    "### 3.2. 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca1009d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price: 715\n",
      "dApple: 2.2\n",
      "dApple_num: 110\n",
      "dOrange: 3.3000000000000003\n",
      "dOrange_num: 165\n",
      "dTax: 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax)  # (4)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)\n",
    "\n",
    "print(\"price:\", int(price))\n",
    "print(\"dApple:\", dapple)\n",
    "print(\"dApple_num:\", int(dapple_num))\n",
    "print(\"dOrange:\", dorange)\n",
    "print(\"dOrange_num:\", int(dorange_num))\n",
    "print(\"dTax:\", dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35cac95",
   "metadata": {},
   "source": [
    "## 4. 활성화 함수 계층 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71594447",
   "metadata": {},
   "source": [
    "### 4.1. ReLU 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83272a1",
   "metadata": {},
   "source": [
    "활성화 함수로 사용되는 **ReLU**의 수식과 이를 미분한 값은 다음과 같다.\n",
    "\n",
    "$$y = \\begin{pmatrix} x (x>0) \\\\ 0 (x<=0) \\end{pmatrix}$$\n",
    "\n",
    "$$\\frac{\\theta y}{\\theta x} = \\begin{pmatrix} 1 (x>0) \\\\ 0 (x <=0) \\end{pmatrix}$$\n",
    "\n",
    "ReLU를 구현한 코드는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e088d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb2347ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input \n",
      " [[ 1.  -0.5]\n",
      " [-2.   3. ]] \n",
      " ReLU \n",
      " [[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "mask = (x <= 0)\n",
    "print(\"input\", \"\\n\", x, \"\\n\", \"ReLU\", \"\\n\", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f6d904",
   "metadata": {},
   "source": [
    "mask를 사용하여 mask의 원소가 True인 곳에는 상류에서 전파된 dout을 0으로 설정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadcc67a",
   "metadata": {},
   "source": [
    "### 4.2.Sigmoid 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7855b2",
   "metadata": {},
   "source": [
    "활성화 함수로 사용되는 **Sigmoid**의 수식은 다음과 같다.\n",
    "    \n",
    "$y = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "이 식을 국소적 계산을 활용하여 표현하면 다음과 같다.\n",
    "\n",
    "$(1/(((x * -1)exp)+1))$\n",
    "\n",
    "위 식을 살펴보면 **\\*, exp, +, /** 순서로 계산된다. 위 식을 미분하면 다음과 같은 결과와 그 값의 정리값이 나온다.\n",
    "\n",
    "$\\frac{\\theta L}{\\theta y}y^2 e^{-x} = \\frac{\\theta L}{\\theta y}y(1-y)$\n",
    "\n",
    "위 식을 코드로 구현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11fb5428",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cef939",
   "metadata": {},
   "source": [
    "## 5. Affine/Softmax 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f475db48",
   "metadata": {},
   "source": [
    "### 5.1. Affine 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f93a3a2",
   "metadata": {},
   "source": [
    "신경망의 순전파 때 수행하는 행렬의 곱은 기하학에서 **어파인 변환**이라고 불린다. 이를 구현한 계층을 **Affine 계층**이라고 한다.\n",
    "\n",
    "위에서 본 덧셈의 역전파는 스칼라 값으로 계산했다. 하지만 행렬의 곲에서는 대응하는 차원의 원소를 일치시켜야 한다. 이를 위해 행렬의 곱의 역전파를 진행할 때는 다른 값의 **전치 행렬**을 곱하여 계산한다. 다음과 같이 계산된다.\n",
    "\n",
    "$Y = XW$\n",
    "\n",
    "$\\frac{\\theta L}{\\theta X} = \\frac{\\theta L}{\\theta Y} * W^T$\n",
    "\n",
    "이를 코드로 구현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11b7cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca4837",
   "metadata": {},
   "source": [
    "### 5.2. Softmax-with-Loss 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c53539",
   "metadata": {},
   "source": [
    "Softmax 계층은 입력 값을 정규화(출력의 합이 1이 되도록 변형)하여 출력한다. 이 softmax 계층에 손실 함수인 고차 엔트로피 오차를 포함한 계층을 **Softmax-with-Loss 계층**이라고 한다. 이는 다음과 같이 구현된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49fd37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02bb8cf",
   "metadata": {},
   "source": [
    "## 6. 오차역전파 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a53eb",
   "metadata": {},
   "source": [
    "### 6.1. 신경망 학습의 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df7147d",
   "metadata": {},
   "source": [
    "1. 미니배치\n",
    "- 훈련 데이터 중 일부를 선별한다.\n",
    "2. 기울기 산출\n",
    "- 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다.\n",
    "3. 매개변수 갱신\n",
    "- 가중치 매개변수를 기울기 방향으로 조금 갱신\n",
    "4. 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd696c9",
   "metadata": {},
   "source": [
    "### 6.2. 오차역전파법을 적용한 신경망 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24c4a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23243271",
   "metadata": {},
   "source": [
    "위 코드에서 가장 중요한 부분은 신경망의 계층을 **OrderedDict**에 보관한다는 것이다. OrderedDict는 **순서가 있는 딕셔너리**이다. 순서가 있기 때문에 추가한 순서대로 각 계층의 forward() 메서드 호출만으로 간단하게 처리할 수 있다. \n",
    "\n",
    "이와 같이 신경망의 구성 요소를 **계층**으로 구현하므로서 간단하게 신경망을 구축할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3118825",
   "metadata": {},
   "source": [
    "### 6.3.오차역전파법으로 구한 기울기 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68de392",
   "metadata": {},
   "source": [
    "수치 미분은 구현하기 쉽지만 종종 오류가 난다. 그렇기 때문에 수치 미분의 결과와 오차역전파법의 결과를 비교하는 **검증**을 진행해야한다.이를 위해 두 방식으로 구한 기울기를 비교하는 방법을 **기울기 확인(Gradient check)** 라고 한다. 이는 다음과 같이 구현된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49c79ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:3.951573736117426e-10\n",
      "b1:2.4164602457256846e-09\n",
      "W2:5.736617895264089e-09\n",
      "b2:1.39295182836896e-07\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir) \n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 절대 오차의 평균을 구한다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd9416",
   "metadata": {},
   "source": [
    "### 6.4. 오차역전파법을 사용한 학습 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c25d5dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14955 0.1505\n",
      "0.9057 0.9095\n",
      "0.9214166666666667 0.9254\n",
      "0.9374166666666667 0.9364\n",
      "0.9465 0.9448\n",
      "0.9515666666666667 0.9485\n",
      "0.9565333333333333 0.9533\n",
      "0.9606333333333333 0.9569\n",
      "0.9639333333333333 0.9592\n",
      "0.96595 0.9607\n",
      "0.9692 0.9636\n",
      "0.9714166666666667 0.9642\n",
      "0.9722666666666666 0.9659\n",
      "0.9747666666666667 0.9671\n",
      "0.9765166666666667 0.9682\n",
      "0.9774333333333334 0.9692\n",
      "0.9788333333333333 0.9693\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
